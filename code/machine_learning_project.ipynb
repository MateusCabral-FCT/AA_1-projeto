{
 "cells": [
  {
   "cell_type": "code",
   "id": "88209640",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import sns\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn. preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn. linear_model import Ridge, RidgeCV\n",
    "from sklearn. compose import ColumnTransformer\n",
    "import time\n",
    "\n",
    "WITH_VELOCITIES = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24466955",
   "metadata": {},
   "source": [
    "data = pd.read_csv(\"../data/X_train.csv\")\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf9be039",
   "metadata": {},
   "source": [
    "data.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa48e41b",
   "metadata": {},
   "source": [
    "idx = np.hstack((0,data[data.t == 10].index.values +1))\n",
    "idx. shape, data. t.min(), data. t.max()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42dd9f0f",
   "metadata": {},
   "source": [
    "k = np.random. randint(idx. shape [0])\n",
    "print(k)\n",
    "pltidx = range(idx[k],257+idx[k])\n",
    "pltsquare = idx [k]\n",
    "plt.plot(data.x_1[pltidx],data.y_1[pltidx])\n",
    "plt.plot(data.x_2[pltidx],data.y_2[pltidx])\n",
    "plt.plot(data.x_3[pltidx],data.y_3[pltidx])\n",
    "\n",
    "plt.plot(data.x_1[pltsquare],data.y_1[pltsquare],'s')\n",
    "plt.plot(data.x_2[pltsquare],data.y_2[pltsquare], 's' )\n",
    "plt.plot(data.x_3[pltsquare],data.y_3[pltsquare],'s')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8ac56d878b34578",
   "metadata": {},
   "source": [
    "Remove all the found data anomalies\n",
    "\n",
    "#1 - Removed the line with all os"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e42c2b64bd1aa15",
   "metadata": {},
   "source": [
    "data_without_anomalies = data.loc[~((data.x_1 == 0) & (data.y_1 == 0) & (data.v_x_1 == 0) & (data.v_y_1 == 0) &\n",
    "                  (data.x_2 == 0) & (data.y_2 == 0) & (data.v_x_2 == 0) & (data.v_y_2 == 0) &\n",
    "                  (data.x_3 == 0) & (data.y_3 == 0) & (data.v_x_3 == 0) & (data.v_y_3 == 0))]\n",
    "data_without_anomalies = data_without_anomalies.reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c028ae79",
   "metadata": {},
   "source": [
    "def split_by_initial_positions(data, number_of_trajectories = 4998, test_size=0.1, val_size=0.1):\n",
    "\n",
    "    starts = data.index[np.equal(data[\"t\"], 0.0)].to_list()\n",
    "    starts.append(len(data))\n",
    "\n",
    "    print(\"Number of trajectories:\", len(starts)-1)\n",
    "\n",
    "    all_trajectories = []\n",
    "    for i in range(len(starts)-1):\n",
    "        all_trajectories.append(data.iloc[starts[i]:starts[i+1], :])\n",
    "\n",
    "    np.random.shuffle(all_trajectories)\n",
    "\n",
    "    n_val   = int(number_of_trajectories * val_size)\n",
    "    n_test  = int(number_of_trajectories * test_size)\n",
    "    n_train = number_of_trajectories - n_val - n_test\n",
    "\n",
    "    print(\"Train/Val/Test sizes (in trajectory amount):\", n_train, n_val, n_test)\n",
    "\n",
    "    print(len(all_trajectories), \"trajectories in total\")\n",
    "\n",
    "    train_data = pd.concat(all_trajectories[:n_train], ignore_index=True)\n",
    "    val_data   = pd.concat(all_trajectories[n_train:n_train+n_val],ignore_index=True)\n",
    "    test_data  = pd.concat(all_trajectories[n_train+n_val:n_train+n_val+n_test],ignore_index=True)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa455b0b",
   "metadata": {},
   "source": [
    "train_data, val_data, test_data = split_by_initial_positions(data_without_anomalies,4998)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e4dcab91ce50e3",
   "metadata": {},
   "source": [
    " Task 1.2 - Learn the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ccf76546789626a",
   "metadata": {},
   "source": [
    "def create_splits(data_for_splts):\n",
    "\n",
    "    data_with_intial_positions = data_for_splts.copy()\n",
    "\n",
    "    initial = data.query(\"t == 0.0\")[['Id','x_1','y_1','x_2','y_2','x_3','y_3']].copy()\n",
    "    initial = initial.rename(columns={\n",
    "        'x_1': 'x0_1', 'y_1': 'y0_1',\n",
    "        'x_2': 'x0_2', 'y_2': 'y0_2',\n",
    "        'x_3': 'x0_3', 'y_3': 'y0_3',\n",
    "    })\n",
    "\n",
    "    if WITH_VELOCITIES:\n",
    "        initial_velocities = data.query(\"t == 0.0\")[['Id','v_x_1','v_y_1','v_x_2','v_y_2','v_x_3','v_y_3']].copy()\n",
    "        initial = initial.merge(initial_velocities, on='Id', how='left')\n",
    "        initial = initial.rename(columns={\n",
    "            'v_x_1': 'v_x_0_1', 'v_y_1': 'v_y_0_1',\n",
    "            'v_x_2': 'v_x_0_2', 'v_y_2': 'v_y_0_2',\n",
    "            'v_x_3': 'v_x_0_3', 'v_y_3': 'v_y_0_3',\n",
    "        })\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        data_with_intial_positions.sort_values('Id'),\n",
    "        initial.sort_values('Id'),\n",
    "        on='Id',\n",
    "        direction='backward',\n",
    "        allow_exact_matches=True\n",
    "    )\n",
    "\n",
    "    if not WITH_VELOCITIES:\n",
    "        x_data = merged[['t','x0_1', 'y0_1', 'x0_2', 'y0_2', 'x0_3', 'y0_3']]\n",
    "    else:\n",
    "        x_data = merged[['t','x0_1', 'y0_1', 'x0_2', 'y0_2', 'x0_3', 'y0_3','v_x_0_1', 'v_y_0_1', 'v_x_0_2', 'v_y_0_2', 'v_x_0_3', 'v_y_0_3']]\n",
    "\n",
    "    y_data = merged[['x_1', 'y_1', 'x_2', 'y_2', 'x_3', 'y_3']]\n",
    "\n",
    "    return x_data, y_data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9b41fda885d5198",
   "metadata": {},
   "source": [
    "def plot_y_yhat(y_test,y_pred, plot_title = \"plot\"):\n",
    "    labels = ['x_1','y_1','x_2','y_2','x_3','y_3']\n",
    "\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    MAX = 500\n",
    "\n",
    "    if len(y_test) > MAX:\n",
    "        idx = np.random.choice(len(y_test),MAX, replace=False)\n",
    "    else:\n",
    "        idx = np.arange(len(y_test))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(6):\n",
    "        x0 = np.min(y_test[idx,i])\n",
    "        x1 = np.max(y_test[idx,i])\n",
    "        plt.subplot(3,2,i+1)\n",
    "        plt.scatter(y_test[idx,i],y_pred[idx,i])\n",
    "        plt.xlabel('True '+labels[i])\n",
    "        plt.ylabel('Predicted '+labels[i])\n",
    "        plt.plot([x0,x1],[x0,x1],color='red')\n",
    "        plt.axis('square')\n",
    "    plt.savefig(plot_title+'.pdf')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bba28ed31446932",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_train, y_train = create_splits(train_data)\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(),LinearRegression())\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "x_val, y_val = create_splits(val_data)\n",
    "y_pred = pipeline.predict(x_val)\n",
    "\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(\"MSE:\",mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\",rmse)\n",
    "\n",
    "x_test, y_test = create_splits(test_data)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "plot_y_yhat(y_test,y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6865157e9d54ef6d",
   "metadata": {},
   "source": [
    "test_data = pd.read_csv(\"../data/X_test.csv\")\n",
    "\n",
    "test_data = test_data.drop(columns=['Id'])\n",
    "\n",
    "if WITH_VELOCITIES:\n",
    "    test_data['v_x_0_1'] = 0\n",
    "    test_data['v_y_0_1'] = 0\n",
    "    test_data['v_x_0_2'] = 0\n",
    "    test_data['v_y_0_2'] = 0\n",
    "    test_data['v_x_0_3'] = 0\n",
    "    test_data['v_y_0_3'] = 0\n",
    "\n",
    "y_testData = pipeline.predict(test_data)\n",
    "\n",
    "submission = pd.DataFrame(y_testData, columns=['x_1','y_1','x_2','y_2','x_3','y_3'])\n",
    "submission.insert(0, 'Id', range(0, len(submission)))\n",
    "\n",
    "submission.to_csv('baseline-model.csv',index=False)"
   ],
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Regularization Methods\n",
    "\n",
    "We tried Ridge(L2), Lasso(L1) and ElasticNet(L1+L2) regularization methods. The best results were obtained with Ridge regression.\n",
    "\n",
    "Not only were the best results obtained with Ridge regression, but also the training time was significantly lower than with Lasso and ElasticNet allowing us to explore higher polynomial degrees and with more data points (trajectories)."
   ],
   "id": "7f661bfa0027b54c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Best Degree\n",
    "\n",
    "We tried polynomial degrees from 1 to 8. The best results were obtained with degree 4."
   ],
   "id": "784afddf67e62933"
  },
  {
   "cell_type": "code",
   "id": "d7c12c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:54:13.129578Z",
     "start_time": "2025-10-07T15:52:51.620636Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, MultiTaskElasticNetCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def validate_poly_regression(X_train, y_train, X_val, y_val, \n",
    "                             regressor=None, degrees=range(4,5), max_features=None):\n",
    "\n",
    "    if regressor is None:\n",
    "        regressor = LinearRegression()\n",
    "\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model = None\n",
    "\n",
    "\n",
    "    for d in degrees:\n",
    "        pipe = Pipeline([\n",
    "            (\"poly\", PolynomialFeatures(degree=d, include_bias=False)),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"regressor\", RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5, scoring='neg_mean_squared_error'))\n",
    "            #(\"regressor\",MultiTaskLassoCV(alphas=[0.1, 1.0, 10.0], cv=5, max_iter=1000))\n",
    "            #(\"regressor\", MultiTaskElasticNetCV(alphas=[0.1,1.0,10.0], l1_ratio=[0.5], cv=3, max_iter=10000))\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipe.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "        n_feats = pipe.named_steps[\"poly\"].n_output_features_\n",
    "        print(f\"Degree {d}: RMSE={rmse:.4f}, Features={n_feats}\")\n",
    "\n",
    "        if rmse < best_rmse and (max_features is None or n_feats <= max_features):\n",
    "            best_rmse = rmse\n",
    "            best_model = pipe\n",
    "\n",
    "    return best_model, best_rmse\n",
    "\n",
    "poly_train_data, poly_val_data, poly_test_data = split_by_initial_positions(data_without_anomalies,4998)\n",
    "poly_x_train, poly_y_train = create_splits(poly_train_data)\n",
    "poly_x_val, poly_y_val = create_splits(poly_val_data)\n",
    "best_model, best_rmse = validate_poly_regression(poly_x_train, poly_y_train, poly_x_val, poly_y_val)\n",
    "print(\"Best RMSE:\", best_rmse)\n",
    "\n",
    "test_data_poly = pd.read_csv(\"../data/X_test.csv\")\n",
    "\n",
    "test_data_poly = test_data_poly.drop(columns=['Id'])\n",
    "\n",
    "if WITH_VELOCITIES:\n",
    "    test_data['v_x_0_1'] = 0\n",
    "    test_data['v_y_0_1'] = 0\n",
    "    test_data['v_x_0_2'] = 0\n",
    "    test_data['v_y_0_2'] = 0\n",
    "    test_data['v_x_0_3'] = 0\n",
    "    test_data['v_y_0_3'] = 0\n",
    "\n",
    "y_testData_poly = best_model.predict(test_data_poly)\n",
    "\n",
    "print(y_testData_poly.shape)\n",
    "\n",
    "submission_poly = pd.DataFrame(y_testData_poly, columns=['x_1','y_1','x_2','y_2','x_3','y_3'])\n",
    "submission_poly.insert(0, 'Id', range(0, len(submission_poly)))\n",
    "\n",
    "submission_poly.to_csv('polynomial_submission.csv',index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trajectories: 4998\n",
      "Train/Val/Test sizes (in trajectory amount): 4000 499 499\n",
      "4998 trajectories in total\n",
      "Degree 4: RMSE=1.2036, Features=329\n",
      "Best RMSE: 1.203587057930673\n",
      "(1041621, 6)\n"
     ]
    }
   ],
   "execution_count": 147
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
